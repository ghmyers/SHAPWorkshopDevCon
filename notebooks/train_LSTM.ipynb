{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## TITLE: Train Models\n",
                "### AUTHOR: Harrison\n",
                "### DATE: 2025-04-23\n",
                "##### DESCRIPTION: Trains Multiple Linear Regression and LSTM model on CAMELS data for discharge prediction\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import logging\n",
                "import sys\n",
                "\n",
                "# Project directory structure\n",
                "PROJECT_DIR = os.path.dirname(os.path.abspath(''))\n",
                "DATA_DIR = os.path.join(PROJECT_DIR, 'data')\n",
                "USGS_DATA_DIR = os.path.join(DATA_DIR, 'raw', 'usgs_streamflow')\n",
                "FORCING_DATA_DIR = os.path.join(DATA_DIR, 'raw', 'basin_mean_forcing', 'nldas')\n",
                "STATIC_DATA_DIR = os.path.join(DATA_DIR, 'raw', 'basin_metadata')\n",
                "FIGURE_DIR = os.path.join(PROJECT_DIR, 'outputs', 'figures')\n",
                "MODEL_DIR = os.path.join(PROJECT_DIR, 'models')\n",
                "os.chdir(PROJECT_DIR)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Preprocessed data not found. Running preprocessing script.\n",
                        "STAID not found: '01150900'\n"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "\n",
                "# try:\n",
                "#     master_df = pd.read_csv(os.path.join(DATA_DIR, 'processed', 'CAMELS.csv'), index_col=0)\n",
                "#     master_df.index = pd.to_datetime(master_df.index)\n",
                "# except:\n",
                "print(\"Preprocessed data not found. Running preprocessing script.\")\n",
                "# Create dictionary for storing data\n",
                "data_dict = {}\n",
                "\n",
                "# Define column names\n",
                "columns = ['STAID', 'YEAR', 'MONTH', 'DAY', 'Q', 'QAQC']\n",
                "# Read in all data\n",
                "for huc in os.listdir(USGS_DATA_DIR):\n",
                "    for basin_data in os.listdir(os.path.join(USGS_DATA_DIR, huc)):\n",
                "        # Read in fixed width .txt file as a dataframe\n",
                "        basin_df = pd.read_fwf(os.path.join(USGS_DATA_DIR, huc, basin_data), \n",
                "                            header=None, names=columns,\n",
                "                            dtype={'STAID': str, 'QAQC': str})\n",
                "        # Convert date columns to datetime and set as index\n",
                "        basin_df['DATE'] = pd.to_datetime(basin_df[['YEAR', 'MONTH', 'DAY']])\n",
                "        basin_df.set_index('DATE', inplace=True)\n",
                "        # Drop unnecessary columns\n",
                "        basin_df.drop(columns=['YEAR', 'MONTH', 'DAY', 'QAQC'], inplace=True)\n",
                "        # Add dataframe to dictionary\n",
                "        data_dict[basin_df['STAID'].iloc[0]] = basin_df\n",
                "        \n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Read in forcing data\n",
                "for huc in os.listdir(FORCING_DATA_DIR):\n",
                "    for basin_data in os.listdir(os.path.join(FORCING_DATA_DIR, huc)):\n",
                "        # Get basin number\n",
                "        basin_no = basin_data.split('_')[0]\n",
                "        # Read in dataframe \n",
                "        basin_df = pd.read_csv(os.path.join(FORCING_DATA_DIR, huc, basin_data), skiprows=3, sep=r'\\s+')\n",
                "        basin_df.rename(columns={'Mnth': 'Month'}, inplace=True)\n",
                "        # Convert date columns to datetime and set as index\n",
                "        basin_df['DATE'] = pd.to_datetime(basin_df[['Year', 'Month', 'Day']])\n",
                "        basin_df.set_index('DATE', inplace=True)\n",
                "        # Drop unnecessary columns\n",
                "        basin_df.drop(columns=['Year', 'Month', 'Day', 'Hr'], inplace=True)\n",
                "        \n",
                "        # Make sure all dtypes are numeric\n",
                "        for dtype in basin_df.dtypes:\n",
                "            assert dtype != object\n",
                "            \n",
                "        # Concatenate forcing dataframe with discharge dataframe\n",
                "        try:\n",
                "            data_dict[basin_no] = pd.concat([data_dict[basin_no], basin_df], axis=1)\n",
                "        except Exception as e:\n",
                "            print(f\"STAID not found: {e}\")\n",
                "            \n",
                "# Read in annual average hydrometeorological data\n",
                "HMET_STATIC_COLS = ['HUC', 'STAID', 'Annual Runoff (mm d-1)', 'Annual Precip (mm d-1)', 'Annual PET (mm d-1)', 'Annual Temp (C)']\n",
                "STATIC_COLS = ['HUC', 'STAID', 'DA (km2)', 'Elevation (m)', 'Slope (m km-1)', 'Frac Forest (%)']\n",
                "hmet_static_df = pd.read_csv(os.path.join(STATIC_DATA_DIR, 'basin_annual_hydrometeorology_characteristics_nldas.txt'), \n",
                "                            sep=r'\\s+', names=HMET_STATIC_COLS, dtype={'STAID': str}, header=0)\n",
                "\n",
                "static_df = pd.read_csv(os.path.join(STATIC_DATA_DIR, 'basin_physical_characteristics.txt'), sep=r'\\s+',\n",
                "                        names=STATIC_COLS, dtype={'STAID': str}, header=0)\n",
                "\n",
                "# Append static data to respective basin dataframe\n",
                "for basin, df in data_dict.items():\n",
                "    row_hmet = hmet_static_df[hmet_static_df['STAID'] == basin]\n",
                "    row = static_df[static_df['STAID'] == basin]\n",
                "    row_hmet.name = basin\n",
                "    if not row_hmet.empty:\n",
                "        static_values_hmet = row_hmet.iloc[0]\n",
                "        static_values = row.iloc[0]\n",
                "        \n",
                "    # Append HMET static variables\n",
                "    data_dict[basin]['Annual Runoff (mm d-1)'] = static_values_hmet['Annual Runoff (mm d-1)']\n",
                "    data_dict[basin]['Annual Precip (mm d-1)'] = static_values_hmet['Annual Precip (mm d-1)']\n",
                "    data_dict[basin]['Annual PET (mm d-1)'] = static_values_hmet['Annual PET (mm d-1)']\n",
                "    data_dict[basin]['Annual Temp (C)'] = static_values_hmet['Annual Temp (C)']\n",
                "    \n",
                "    # Append static variables\n",
                "    data_dict[basin]['DA (km2)'] = static_values['DA (km2)']\n",
                "    data_dict[basin]['Elevation (m)'] = static_values['Elevation (m)']\n",
                "    data_dict[basin]['Slope (m km-1)'] = static_values['Slope (m km-1)']\n",
                "    data_dict[basin]['Frac Forest (%)'] = static_values['Frac Forest (%)']\n",
                "    \n",
                "# Turn data dict into master dataframe\n",
                "master_df = pd.concat(data_dict.values(), axis=0)\n",
                "master_df.dropna(inplace=True)\n",
                "\n",
                "master_df.head()\n",
                "master_df.to_csv(os.path.join(DATA_DIR, 'processed', 'CAMELS.csv'), index=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import random\n",
                "\n",
                "def train_test_split_evenSites(df, split_pct, seed):\n",
                "    # Set random seed\n",
                "    np.random.seed(seed)\n",
                "    random.seed(seed)\n",
                "    \n",
                "    # Get unique site identifiers\n",
                "    sites = df['STAID'].unique()\n",
                "    \n",
                "    # Store splits here\n",
                "    train_splits = []\n",
                "    test_splits  = []\n",
                "    \n",
                "    for site in sites:\n",
                "        temp_df = df[df['STAID'] == site]\n",
                "        split_ind = int(np.floor((1-split_pct)*int(len(temp_df))))\n",
                "        start_ind = np.random.randint(0, len(temp_df) - split_ind)\n",
                "        end_ind = start_ind + split_ind\n",
                "        test_df = temp_df.iloc[start_ind:end_ind, :]\n",
                "        train_df = pd.concat([temp_df.iloc[:start_ind, :], temp_df.iloc[end_ind:, :]])\n",
                "        train_splits.append(train_df)\n",
                "        test_splits.append(test_df)\n",
                "\n",
                "    # Zip lists together, shuffle them, then unzip them\n",
                "    zipped_list = list(zip(train_splits, test_splits))\n",
                "    random.shuffle(zipped_list)\n",
                "    train_splits, test_splits = zip(*zipped_list)\n",
                "    \n",
                "    Train = pd.concat(train_splits)\n",
                "    Test = pd.concat(test_splits)\n",
                "    \n",
                "    Train.drop(\"STAID\", axis=1, inplace=True)\n",
                "    Test.drop('STAID', axis=1, inplace=True)\n",
                "    \n",
                "    X_train = Train.drop('Q', axis=1)\n",
                "    y_train = Train['Q']\n",
                "    X_test = Test.drop('Q', axis=1)\n",
                "    y_test = Test['Q']\n",
                "    \n",
                "    return X_train, y_train, X_test, y_test\n",
                "\n",
                "X_train, y_train, X_test, y_test = train_test_split_evenSites(master_df, 0.8, 42)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "(6751320, 15) (6751320,) (1687180, 15) (1687180,)\n"
                    ]
                }
            ],
            "source": [
                "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.preprocessing import MinMaxScaler\n",
                "import tensorflow as tf\n",
                "\n",
                "feat_scaler = MinMaxScaler()\n",
                "X_train = pd.DataFrame(\n",
                "    feat_scaler.fit_transform(X_train),\n",
                "    columns=X_train.columns,\n",
                "    index=X_train.index\n",
                ")\n",
                "\n",
                "y_scaler = MinMaxScaler()\n",
                "y_train = pd.Series(y_scaler.fit_transform(y_train.values.reshape(-1,1)).squeeze(),\n",
                "    index=y_train.index, name='Q')\n",
                "\n",
                "# apply the scalers to the validation set\n",
                "X_val  = pd.DataFrame(feat_scaler.transform(X_test),\n",
                "                      columns=X_train.columns, \n",
                "                      index=X_test.index)\n",
                "\n",
                "y_val  = pd.Series(y_scaler.transform(y_test.values.reshape(-1,1)).squeeze(),\n",
                "                   index=y_test.index, name='Q')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Train multiple linear regression model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "R^2: -0.03969659402824233\n"
                    ]
                }
            ],
            "source": [
                "from sklearn.linear_model import LinearRegression\n",
                "from sklearn.metrics import mean_squared_error, r2_score\n",
                "\n",
                "model = LinearRegression()\n",
                "model.fit(X_train, y_train)\n",
                "y_pred = model.predict(X_val)\n",
                "\n",
                "print(f\"R^2: {r2_score(y_scaler.inverse_transform(np.array(y_val).reshape(-1,1)),\n",
                "      y_scaler.inverse_transform(np.array(y_pred).reshape(-1, 1)))}\")\n",
                "\n",
                "# Save trained model\n",
                "import joblib\n",
                "import pickle\n",
                "fpath = os.path.join(MODEL_DIR, 'linear_regression_weights.npz')\n",
                "np.savez(\n",
                "    fpath,\n",
                "    coef=model.coef_.astype(np.float32),     \n",
                "    intercept=np.asarray(model.intercept_, dtype=np.float32),\n",
                "    n_features_in=np.int32(model.n_features_in_),\n",
                "    feature_names_in=getattr(model, \"feature_names_in_\", None)\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "def data_generator(X, y, window_length, batch_size):\n",
                "    X_array = X.to_numpy(dtype=np.float32)\n",
                "    y_array = y.to_numpy(dtype=np.float32)\n",
                "    n_samples = len(X)\n",
                "\n",
                "    while True:\n",
                "        indices = np.arange(n_samples - window_length + 1)\n",
                "        X_batch = []\n",
                "        y_batch = []\n",
                "        \n",
                "        # for site, indices in valid_sequences.items():\n",
                "        for i in indices:\n",
                "            # Get start and end dates of sequence\n",
                "            sequence_start = X.index[i]\n",
                "            sequence_end = X.index[i + window_length - 1]\n",
                "            if (sequence_end - sequence_start).days == window_length - 1:\n",
                "                X_seq = X_array[i:i + window_length]\n",
                "                y_seq = y_array[i + window_length - 1]\n",
                "                \n",
                "                X_batch.append(X_seq)\n",
                "                y_batch.append(y_seq)\n",
                "                \n",
                "                if len(X_batch) == batch_size:\n",
                "                    y_batch_array = np.array(y_batch)\n",
                "                    yield np.array(X_batch), np.array(y_batch)\n",
                "                    X_batch, y_batch = [], []\n",
                "            \n",
                "        if X_batch and y_batch:  # If there are any remaining sequences not yielded yet\n",
                "            yield np.array(X_batch), np.array(y_batch)\n",
                "\n",
                "def get_steps_per_epoch(X, window_length, batch_size):\n",
                "    \"\"\"A function to count the number of valid sequences in the training data, X\n",
                "\n",
                "    Args:\n",
                "        X (pd.DataFrame or np.array): The training data\n",
                "        window_length (int): The window length for input to the LSTM\n",
                "        batch_size (int): The batch size for the LSTM\n",
                "\n",
                "    Returns:\n",
                "        int: the number of steps per epoch\n",
                "    \"\"\"\n",
                "    n_samples = len(X)\n",
                "    indices = np.arange(n_samples - window_length + 1)\n",
                "    valid_sequence_count = 0\n",
                "\n",
                "    for i in indices:\n",
                "        sequence_start = X.index[i]\n",
                "        sequence_end = X.index[i + window_length - 1]\n",
                "        if (sequence_end - sequence_start).days == window_length - 1:\n",
                "            valid_sequence_count += 1\n",
                "    print(valid_sequence_count)\n",
                "    steps_per_epoch = valid_sequence_count // batch_size\n",
                "    \n",
                "    return steps_per_epoch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import keras\n",
                "import tensorflow as tf\n",
                "from keras import layers, metrics\n",
                "# from sklearn.metrics import mean_squared_error, r2_score\n",
                "\n",
                "def build_model(hidden_layers, hidden_units, optimizer, window_length, \n",
                "                n_features, lr, dropout=0,  activation_dense=\"relu\"):\n",
                "    loss_fn = 'mean_squared_error'\n",
                "    optimizer = optimizer\n",
                "    model = keras.Sequential()\n",
                "    model.add(layers.Input(shape=(window_length, n_features)))\n",
                "    for i in range(hidden_layers):\n",
                "        if i != hidden_layers - 1:\n",
                "            model.add(layers.LSTM(units=hidden_units, return_sequences=True))\n",
                "            model.add(layers.Dropout(dropout))\n",
                "        else:\n",
                "            model.add(layers.LSTM(units=hidden_units))\n",
                "            model.add(layers.Dropout(dropout))\n",
                "\n",
                "    model.add(layers.Dense(units=1, activation=activation_dense))\n",
                "    \n",
                "    model.compile(loss=loss_fn, optimizer=optimizer, metrics=[metrics.R2Score(), metrics.RootMeanSquaredError(), metrics.MeanAbsoluteError()])\n",
                "    model.summary()\n",
                "              \n",
                "    return model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Hyperparameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [],
            "source": [
                "import keras \n",
                "\n",
                "seed = 42\n",
                "hidden_layers = 1\n",
                "hidden_units = 32\n",
                "window_length = 180\n",
                "n_features = len(X_train.columns)\n",
                "dropout = 0.3\n",
                "lr = 0.0001\n",
                "batch_size = 256\n",
                "n_epochs = 150\n",
                "optimizer = keras.optimizers.Adam(learning_rate=lr)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "6511799\n"
                    ]
                }
            ],
            "source": [
                "train_gen = data_generator(X_train, y_train, window_length, batch_size)\n",
                "steps_per_epoch = get_steps_per_epoch(X_train, window_length, batch_size)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1566641\n"
                    ]
                }
            ],
            "source": [
                "test_gen = data_generator(X_test, y_test, window_length, batch_size)\n",
                "val_steps = get_steps_per_epoch(X_test, window_length, batch_size)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
                            "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
                            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
                            "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,144</span> │\n",
                            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
                            "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
                            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
                            "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
                            "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
                            "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
                            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
                            "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m6,144\u001b[0m │\n",
                            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
                            "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
                            "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
                            "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
                            "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,177</span> (24.13 KB)\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,177\u001b[0m (24.13 KB)\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,177</span> (24.13 KB)\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,177\u001b[0m (24.13 KB)\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
                            "</pre>\n"
                        ],
                        "text/plain": [
                            "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "from keras import layers, metrics\n",
                "from sklearn.metrics import mean_squared_error, r2_score\n",
                "\n",
                "model = build_model(hidden_layers, hidden_units, optimizer, window_length, n_features, lr, dropout)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1/150\n",
                        "\u001b[1m25436/25436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1913s\u001b[0m 75ms/step - loss: 1.7974e-04 - mean_absolute_error: 0.0103 - r2_score: -9.1669 - root_mean_squared_error: 0.0132 - val_loss: 1.5980e-04 - val_mean_absolute_error: 0.0103 - val_r2_score: -1.9534 - val_root_mean_squared_error: 0.0126 - learning_rate: 1.0000e-04\n",
                        "Epoch 2/150\n",
                        "\u001b[1m25436/25436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1889s\u001b[0m 74ms/step - loss: 1.5491e-04 - mean_absolute_error: 0.0101 - r2_score: -2.3029 - root_mean_squared_error: 0.0124 - val_loss: 1.5979e-04 - val_mean_absolute_error: 0.0103 - val_r2_score: -1.9532 - val_root_mean_squared_error: 0.0126 - learning_rate: 1.0000e-04\n",
                        "Epoch 3/150\n",
                        "\u001b[1m25436/25436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1683s\u001b[0m 66ms/step - loss: 1.5491e-04 - mean_absolute_error: 0.0101 - r2_score: -2.1822 - root_mean_squared_error: 0.0124 - val_loss: 1.5972e-04 - val_mean_absolute_error: 0.0103 - val_r2_score: -1.9544 - val_root_mean_squared_error: 0.0126 - learning_rate: 1.0000e-04\n",
                        "Epoch 4/150\n",
                        "\u001b[1m25436/25436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1737s\u001b[0m 68ms/step - loss: 1.5493e-04 - mean_absolute_error: 0.0101 - r2_score: -2.1405 - root_mean_squared_error: 0.0124 - val_loss: 1.5980e-04 - val_mean_absolute_error: 0.0103 - val_r2_score: -1.9532 - val_root_mean_squared_error: 0.0126 - learning_rate: 1.0000e-04\n",
                        "Epoch 5/150\n",
                        "\u001b[1m22649/25436\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m2:38\u001b[0m 57ms/step - loss: 1.5427e-04 - mean_absolute_error: 0.0101 - r2_score: -2.1775 - root_mean_squared_error: 0.0124"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "KeyboardInterrupt\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "import tensorflow as tf\n",
                "from keras.callbacks import EarlyStopping\n",
                "\n",
                "keras.utils.set_random_seed(seed)\n",
                "model_name = 'River_LSTM'\n",
                "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min')\n",
                "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=5, min_lr=0.0000001, verbose=1)\n",
                "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(PROJECT_DIR,'models', f'{model_name}.weights.h5'), save_best_only=True, save_weights_only=True, monitor='val_loss', mode='min', verbose=0)\n",
                "\n",
                "callbacks = [earlyStopping, lr_scheduler, checkpoint_callback]\n",
                "history = model.fit(train_gen, steps_per_epoch=steps_per_epoch, batch_size=batch_size, \n",
                "                    epochs=n_epochs, verbose=1, validation_data=test_gen, \n",
                "                    validation_steps=val_steps, callbacks=callbacks)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(6,4), dpi=400)\n",
                "plt.plot(history.history['loss'])\n",
                "plt.plot(history.history['val_loss'])\n",
                "plt.title('LSTM Model Loss')\n",
                "plt.ylabel('Loss (MSE)')\n",
                "plt.xlabel('Epoch')\n",
                "plt.legend(['Train', 'Validation'], loc='upper right')\n",
                "plt.savefig(os.path.join(FIGURE_DIR, 'LSTM_loss.png'))\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "TF_GPU",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
