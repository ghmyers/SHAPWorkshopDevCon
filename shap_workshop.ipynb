{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SHAP Values Workshop\n",
        "\n",
        "Welcome to the **SHAP Values Workshop**! During this session, we’ll explore how to interpret various machine learning model predictions using SHAP (SHapley Additive exPlanations).\n",
        "\n",
        "## Workshop Overview\n",
        "\n",
        "**Length**: 1.5 hours\n",
        "\n",
        "**Introduction to SHAP Values (40 minutes)**\n",
        "   - Overview of interpretability and its importance.\n",
        "   - The theory behind Shapley values.\n",
        "   - Intepreting SHAP plots.\n",
        "   - Real world examples\n",
        "\n",
        "**Break (10 minutes)**\n",
        "\n",
        "**Hands-On SHAP Applications (40 minutes)**\n",
        "   - Walkthrough with a simple multiple linear regression.\n",
        "   - Walkthrough with a neural network (LSTM).\n",
        "   - Walkthrough with a classifier.\n",
        "\n",
        "**We’ll use pretrained models (trained on the CAMELS and GAGESII dataset). You’ll load each model, run the SHAP package to generate explanations, and discuss how to interpret them.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Objective**: For the first two parts of this workshop, we'll use two models (a multiple linear regression and LSTM model) trained on the CAMELS dataset to predict discharge from hydrometeorological and static catchment attribute data. We'll explore the feature importances and attributions in each of these models by utilizing SHAP values.\n",
        "\n",
        "## 1. Data Preprocessing\n",
        "\n",
        "### 1.1 Dataset Overview\n",
        "- Let's do some preliminary exploratory data analysis on the CAMELS dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67b80acf",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Set directories for accessing data, saved SHAP values, and pretrained models\n",
        "DATA_DIR = os.path.join(Path.cwd(), 'data')\n",
        "OUTPUT_DIR = os.path.join(Path.cwd(), \"outputs\")\n",
        "MODEL_DIR = os.path.join(Path.cwd(), \"models\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dad66219",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read in data\n",
        "import pandas as pd\n",
        "df = pd.read_csv(Path(DATA_DIR, 'CAMELS_daymet_cleaned.csv'), index_col=0)\n",
        "df.index = pd.to_datetime(df.index)\n",
        "\n",
        "# Drop STAID from the dataframe\n",
        "try:\n",
        "    df.drop(columns=['STAID'], inplace=True)\n",
        "except:\n",
        "    print(\"STAID already dropped from dataframe\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d957aa9",
      "metadata": {},
      "source": [
        "- Print out the first 10 rows of the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2156539",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef8dc5c5",
      "metadata": {},
      "source": [
        "- Check to make sure there are no NAs present in our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "025002e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df.isna().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "605e9539",
      "metadata": {},
      "source": [
        "- Look at summary statistics of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04c80475",
      "metadata": {},
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0b74b4d",
      "metadata": {},
      "source": [
        "- Look at distributions of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7124ff1b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "df.hist(bins=30, figsize=(14, 14), grid=False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a490018",
      "metadata": {},
      "source": [
        "### 1.2.1 Preprocess data for the Multiple Linear Regression Model\n",
        "- Split dataframe in X (inputs) and y (target), scale input data, and split into training and testing sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f03d784",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "X = df.drop('Q', axis=1)\n",
        "y = df['Q']\n",
        "\n",
        "feat_scaler = StandardScaler()\n",
        "X_sc = pd.DataFrame(\n",
        "    feat_scaler.fit_transform(X),\n",
        "    columns=X.columns,\n",
        "    index=X.index\n",
        ")\n",
        "\n",
        "y_scaler = StandardScaler()\n",
        "y_sc = pd.Series(y_scaler.fit_transform(y.values.reshape(-1,1)).squeeze(),\n",
        "    index=y.index, name='Q')\n",
        "\n",
        "# get mean and scaler value for y_scaler, which we'll use to retranform the SHAP values later\n",
        "y_scaler_mean = y_scaler.mean_[0]\n",
        "y_scaler_scale = y_scaler.scale_[0]\n",
        "\n",
        "# Split the data into training and testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_sc, y_sc, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad17c376",
      "metadata": {},
      "source": [
        "### 1.2.2 Process data for the LSTM model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1b908cb",
      "metadata": {},
      "source": [
        "- Reshape data for LSTM (n_batches, window_length, n_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "298e1ccc",
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.model_building import build_lstm_windows\n",
        "window_length = 60 # days\n",
        "X_rs, y_rs = build_lstm_windows(X_test, y_test, window_length)\n",
        "print(X_rs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Applying SHAP to a Multiple Linear Regression\n",
        "\n",
        "### 2.1 Loading the Pretrained Multiple Linear Regression\n",
        "Use `np.load()` to load a pretrained multiple linear regression model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Load in pre-trained multiple linear regression model\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "linreg = LinearRegression()\n",
        "linreg.coef_ = np.load(Path(MODEL_DIR, 'linreg_model.npz'))['coefficients']\n",
        "linreg.intercept_ = np.load(Path(MODEL_DIR, 'linreg_model.npz'))['intercept']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Running SHAP Explanations\n",
        "We'll use SHAP's `Explainer` for the **Multiple Linear Regression model**. Then we generate `shap_values` for the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shap\n",
        "# Get 10,000 random samples from the dataset\n",
        "np.random.seed(42)\n",
        "idx = np.random.choice(len(X_sc), size=10000, replace=False)\n",
        "X_shap = X_sc.iloc[idx]        \n",
        "y_shap = y_sc.iloc[idx]\n",
        "feature_names = list(X.columns)\n",
        "\n",
        "# Generate shap Explainer object and shap values\n",
        "linear_explainer = shap.Explainer(linreg, X_shap)\n",
        "shap_values = linear_explainer(X_shap)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75bed7f2",
      "metadata": {},
      "source": [
        "- Let's take a look at our raw SHAP values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea7f22e6",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(shap_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a991ba1",
      "metadata": {},
      "source": [
        "The SHAP values are in units of the scaled inputs/output, so we need to rescale the SHAP values so we can interpret them in their original units."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfe81c12",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Multiply by the scaler values\n",
        "shap_values.values *= y_scaler_scale\n",
        "shap_values.base_values *= y_scaler_scale\n",
        "\n",
        "# Add the mean to the base values (model mean) to rescale\n",
        "shap_values.base_values += y_scaler_mean\n",
        "\n",
        "# Rescale the input data\n",
        "shap_values.data = feat_scaler.inverse_transform(shap_values.data)\n",
        "X_shap_untransformed = feat_scaler.inverse_transform(X_shap)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d01c2746",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(shap_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb9fa93e",
      "metadata": {},
      "source": [
        "**Now the SHAP values are in their respective original units.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d5522c6",
      "metadata": {},
      "source": [
        "### 2.3 Visualizing SHAP values\n",
        "- The SHAP package has built in visualizations to help interpret SHAP values. We can examine SHAP values globally or for local predictions.\n",
        "- In this workshop, we'll explore 5 visualization techniques:\n",
        "\n",
        "    1. **Bar Plot** (Feature importances)\n",
        "    \n",
        "    2. **Beeswarm plot** (feature importances and attribution)\n",
        "\n",
        "    3. **Dependence plot** (How a single feature affects predictions)\n",
        "\n",
        "    4. **Force Plot** (balance of forces for a given prediction)\n",
        "\n",
        "    5. **Waterfall Plot** (a waterfall plot is similar to a force plot, but it steps through each feature contribution sequentially)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54eb0ae4",
      "metadata": {},
      "source": [
        "#### 2.3.1 Bar Plots\n",
        "- Compares features by the height of their bars, each bar representing that feature’s average absolute SHAP value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54941bb1",
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.visualization import plot_shap_bar_unscaled\n",
        "\n",
        "plot_shap_bar_unscaled(\n",
        "        shap_values,\n",
        "        feature_names=X.columns,\n",
        "        title=\"Feature Importance for MLR model\"\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd4385a8",
      "metadata": {},
      "source": [
        "- We can check to make sure this generally aligns with the coefficients of our MLR model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64ff947c",
      "metadata": {},
      "outputs": [],
      "source": [
        "coefs = {}\n",
        "for i, col in enumerate(list(X_sc.columns)):\n",
        "    coefs[col] = linreg.coef_[i]\n",
        "coefs = sorted(coefs.items(), key=lambda x: np.abs(x[1]), reverse=True)\n",
        "for var, coef in coefs:\n",
        "    print(f\"MLR coefficient for {var}: {round(coef, 3)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1edc044b",
      "metadata": {},
      "source": [
        "The MLR coefficients generally line up with the feature importances we see from SHAP. We can expect some variability because the SHAP values are calculated over a random subset of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dc26f9c",
      "metadata": {},
      "source": [
        "#### 2.3.2 Beeswarm plots\n",
        "- Packs every SHAP value (from set of data used to generate them) into one figure: each dot shows a single observation, its position shows impact on model predictions (left = negative, right = positive), and its color encodes the raw feature value. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0c18d57",
      "metadata": {},
      "outputs": [],
      "source": [
        "# A feature attribution (beeswarm) plot\n",
        "shap.summary_plot(shap_values, X_shap_untransformed, feature_names=feature_names, cmap='cool')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56b8a0d3",
      "metadata": {},
      "source": [
        "#### 2.3.3 Dependence plots\n",
        "- A scatter plot of a feature’s raw value (x-axis) versus its SHAP value (y-axis). It traces how changing that feature pushes the prediction up or down across the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ba3f2b0",
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.visualization import plot_all_dependence\n",
        "\n",
        "fig, axes = plot_all_dependence(\n",
        "                shap_values       = shap_values,       \n",
        "                X_values          = X_shap_untransformed,\n",
        "                feature_names     = feature_names,\n",
        "                n_cols            = 4,\n",
        "                point_size        = 7,\n",
        "                alpha             = 0.6,\n",
        "                cmap_name         = \"tab20\")\n",
        "plt.show()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b43596f",
      "metadata": {},
      "source": [
        "#### 2.3.4 Force Plots\n",
        "- Breaks one individual prediction into colored blocks along a line: blocks to the right push the prediction higher, blocks to the left push it lower."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32f757a1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get a random index to show a force plot interpretation\n",
        "np.random.seed(42)\n",
        "row = np.random.randint(0, len(X_shap))\n",
        "\n",
        "# Round features for display\n",
        "feat_raw = X_shap_untransformed[row, :]\n",
        "feat_round = feat_raw.round(2)\n",
        "\n",
        "# Plot force plot\n",
        "shap.initjs()               \n",
        "shap.force_plot(\n",
        "        base_value = shap_values.base_values[row],\n",
        "        shap_values = shap_values.values[row],\n",
        "        features    = feat_round,\n",
        "        feature_names = feature_names,\n",
        "        figsize=(20, 4),\n",
        "        matplotlib = True              \n",
        ")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ff7eb28",
      "metadata": {},
      "source": [
        "#### 2.3.5 Waterfall plots\n",
        "- Starts at the model’s expected value and adds (or subtracts) feature contributions step-by-step, largest to smallest. The cumulative “waterfall” shows exactly how the prediction is reached feature by feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abe89da1",
      "metadata": {},
      "outputs": [],
      "source": [
        "shap.plots.waterfall(shap_values[row], max_display=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Neural Network (LSTM) with SHAP\n",
        "\n",
        "- Now we'll demonstrate generating and interpreting SHAP values with a Neural Network (LSTM) model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "525b204a",
      "metadata": {},
      "source": [
        "### 3.1 Build LSTM model, load pretrained weights, and get SHAP values\n",
        "- Generating SHAP values with an LSTM takes longer than with a simpler model, so we'll load them in from a file. If you're curious how to implement this from scratch, check out the `get_lstm_shap_vals()` and `load_npz_array()` functions from the `/src` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abfa61ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.model_building import get_lstm_shap_vals, load_npz_array\n",
        "\n",
        "WEIGHTS_PATH = MODEL_DIR + \"/LSTM_weights_daymet.weights.h5\"\n",
        "SHAP_PATH = OUTPUT_DIR + \"/shap_daymet.npz\"\n",
        "BASEVALS_PATH = OUTPUT_DIR + \"/basevals_daymet.npz\"\n",
        "SHAP_DATA_PATH = OUTPUT_DIR + \"/shap_data_daymet.npz\"\n",
        "\n",
        "try:\n",
        "    shap_values_2d = load_npz_array(SHAP_PATH, key=\"shap_values_2d\")\n",
        "    base_values_2d = load_npz_array(BASEVALS_PATH, key=\"base_values_2d\")\n",
        "    data_2d = load_npz_array(SHAP_DATA_PATH, key=\"data_2d\")\n",
        "except:\n",
        "    shap_values_2d, shap_base_values_2d, data_2d = get_lstm_shap_vals(weights_path=WEIGHTS_PATH, hidden_units=8, window_length=60, n_cols=X_train.shape[1], \n",
        "                                                                      n_batches=1000, X_rs=X_rs, feat_scaler=feat_scaler, y_scaler=y_scaler, shap_outpath=SHAP_PATH,\n",
        "                                                                      basevals_outpath=BASEVALS_PATH, shap_data_outpath=SHAP_DATA_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d51d1fb1",
      "metadata": {},
      "source": [
        "### 3.2 Visualize LSTM SHAP values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2f6fe73",
      "metadata": {},
      "source": [
        "#### 3.2.1 Beeswarm plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adfa6bbe",
      "metadata": {},
      "outputs": [],
      "source": [
        "shap.summary_plot(shap_values_2d, data_2d, feature_names=feature_names, cmap='cool')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b919e277",
      "metadata": {},
      "source": [
        "How do these feature importances and attributions align with our expectations, how do they differ? \n",
        "\n",
        "If they do deviate from our expectations, what could be causing the unexpected behavior?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7c78441",
      "metadata": {},
      "source": [
        "#### 3.2.2 Dependence plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3da010ce",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plot_all_dependence(\n",
        "                shap_values       = shap_values_2d,       \n",
        "                X_values          = data_2d,\n",
        "                feature_names     = feature_names,\n",
        "                n_cols            = 4,\n",
        "                point_size        = 7,\n",
        "                alpha             = 0.6,\n",
        "                cmap_name         = \"tab20\")\n",
        "plt.show()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca4a4422",
      "metadata": {},
      "source": [
        "How is the dependence plot from the LSTM different from the MLR dependence plot?\n",
        "\n",
        "What does this tell you about the behavior of these models?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab6dedc6",
      "metadata": {},
      "source": [
        "## 4. Applying SHAP to a Classifier\n",
        "Objective:  Use SHAP values to explore features associated with unique baseflow regimes.\n",
        "\n",
        "### 4.1 Dataset Overview\n",
        "- Approximately 800 reference-quality gages across CONUS from GAGES-II dataset.\n",
        "- Watershed characteristics (climatic, geographic, hydrologic, geologic, land use, etc....) for each gage.\n",
        "- Gages clustered into seven clusters based on long-term baseflow behavior.\n",
        "\n",
        "### 4.2 Methodology\n",
        "- Random Forest Classification model used to predict a gage's cluster based on watershed characteristics.\n",
        "- SHAP explanation object generated on the classification model.\n",
        "\n",
        "See:    *van der Heijden, R., Dadkhah, A., Hamshaw, S., Ghazanfari, E., Dewoolkar, M., Jones, N., Williams, G., Prabhakar, C., Rizzo, D.M.\n",
        "        \"Identifying and interpreting physical processes and NWM prediction bias associated with baseflow regimes across the CONUS\".\n",
        "        Water Resources Research (under review)*\n",
        "\n",
        "\n",
        "Questions / comments: rvanderh@uvm.edu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d10bb35",
      "metadata": {},
      "outputs": [],
      "source": [
        "#%% Import Libraries\n",
        "import pandas as pd\n",
        "import shap\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# set up plot preferences\n",
        "plt.rcParams['font.sans-serif'] = 'Helvetica'\n",
        "plt.rcParams['font.size'] = 12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff73b26e",
      "metadata": {},
      "outputs": [],
      "source": [
        "data = pd.read_csv(os.path.join(DATA_DIR, 'RF_classifier_DevCon.csv'))\n",
        "data.drop(columns=['STAID'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e040185",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Setup\n",
        "y = data['Cluster_H_7']\n",
        "x = data.drop(columns=['Cluster_H_7'])\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
        "\n",
        "# The model parameters have already been tuned\n",
        "tuned_params = {\n",
        "    'n_estimators':         800,\n",
        "    'max_features':         'sqrt',\n",
        "    'max_depth':            70,\n",
        "    'min_samples_split':    2,\n",
        "    'min_samples_leaf':     1,\n",
        "    'bootstrap':            True\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05cafcce",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit model to training data\n",
        "rf = RandomForestClassifier(n_estimators            = tuned_params['n_estimators'],\n",
        "                            max_depth               = tuned_params['max_depth'],\n",
        "                            max_features            = tuned_params['max_features'],\n",
        "                            min_samples_leaf        = tuned_params['min_samples_leaf'],\n",
        "                            min_samples_split       = tuned_params['min_samples_split'],\n",
        "                            bootstrap               = tuned_params['bootstrap'],\n",
        "                            random_state            = 742\n",
        "                            )\n",
        "\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Add cluster number to X_test\n",
        "if 'Cluster_H_7' not in X_test.columns:\n",
        "    X_test = pd.merge(left=X_test, right=data['Cluster_H_7'], left_index=True, right_index=True)\n",
        "else:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c24c3dea",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get feature importances\n",
        "feature_importances = pd.Series(rf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
        "print(feature_importances)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9547ff6a",
      "metadata": {},
      "source": [
        "### 4.3 About the Explanation Object\n",
        "- The explanation object was created only using the test subset of the data\n",
        "- For this exercise, the explanation object only included the 10 most important features.\n",
        "- The size of the explanation object depends on these factors:\n",
        "\n",
        "\n",
        "[# data points : # features : # output classes]\n",
        "\n",
        "[156 data points in the test set : 10 features : 7 output classes]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bf982b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate explanation object and compute SHAP values\n",
        "explainer = shap.Explainer(rf, X_test)\n",
        "sv = explainer(X_test, check_additivity=False)\n",
        "\n",
        "# check the shape of the explanation object\n",
        "sv.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7addb049",
      "metadata": {},
      "source": [
        "### 3.4 About the Beeswarm Plot\n",
        "\n",
        "The beeswarm plot is a collection of features associated with the output class. For a classification model, each output class has it's own beeswarm plot.\n",
        "\n",
        "The features are arranged on the y-axis and ordered by feature importance, with the most importand features appearing at the top.\n",
        "\n",
        "For each feature, all data points are shown (i.e., in this case 156 data points), arranged along the x-axis by their **SHAP value**, with vertical spread indicating density akin to a violin plot.\n",
        "\n",
        "The data points are color-coded based on their **feature value**. Note that the color ramp is scaled to the range of values of each feature individually, thus is shows a relative magnitude (\"high\" or \"low\" feature values).\n",
        "\n",
        "To interrogate the model further, we can look at the actual distribution of feature values (more on this later...)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21a6905e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming shap_values_a and shap_values_b are your SHAP values for two different models or datasets\n",
        "fig = plt.figure(dpi=200)\n",
        "\n",
        "for i in range(7):\n",
        "    ax = fig.add_subplot(2, 4, i+1)\n",
        "    shap.plots.beeswarm(sv[:,:,i],\n",
        "                        axis_color  = 'k',\n",
        "                        color       = plt.get_cmap('cool'),\n",
        "                        alpha       = 0.7,\n",
        "                        order       = shap.Explanation.abs.mean(0),\n",
        "                        color_bar   = False,\n",
        "                        plot_size   = (20,10),\n",
        "                        show        = False,\n",
        "                        s           = 30\n",
        "                        )\n",
        "    \n",
        "    ax.set_title('Cluster {}'.format(str(i+1)))\n",
        "    ax.set_ylim(5.5, 9.5)   # only show top 4 features for clarity in this example\n",
        "    ax.set_xlabel('SHAP Value', fontsize=10)\n",
        "    ax.tick_params(axis='y', which='major', labelsize=10)\n",
        "\n",
        "# spacing adjustments to make things look better\n",
        "plt.subplots_adjust(wspace=0.8, hspace=0.4)\n",
        "\n",
        "# colorbar\n",
        "cax = ax.inset_axes([1.6, 0.5, 0.7, 0.05])\n",
        "img = ax.scatter(x=[0,0], y=[0,0], c=[0,1], cmap='cool')\n",
        "cbar=fig.colorbar(img, ticks=[0, 1],\n",
        "                    fraction = 0.05,\n",
        "                    pad = 0.05,\n",
        "                    cax=cax,\n",
        "                    label='Feature Value',\n",
        "                    orientation='horizontal')\n",
        "cbar.ax.set_xticklabels(['Low', 'High'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88e965b1",
      "metadata": {},
      "source": [
        "### 3.4 KDE Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62611b2a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# These characteristics are for Cluster 2, and you will need to change them manually depending on the cluster you want to plot\n",
        "clust = 2\n",
        "c = '#4776ee'\n",
        "var = ['SNOW_PCT_PRECIP', 'WB5100_JUN_MM', 'WB5100_NOV_MM', 'WB5100_NOV_MM']\n",
        "feat = ['Snow as % Total Precipitation [%]', 'Watershed Runoff - June [mm]', 'Annual Watershed Runoff\\nIntegrated Estimate [mm]', 'Watershed Runoff - November [mm]']\n",
        "\n",
        "anno_coords = [\n",
        "(0.7, 0.7),\n",
        "(0.7, 0.7),\n",
        "(0.7, 0.7),\n",
        "(0.7, 0.7)\n",
        "]\n",
        "dist_coords = [\n",
        "(0.7, 0.4),\n",
        "(0.7, 0.4),\n",
        "(0.7, 0.4),\n",
        "(0.7, 0.4)\n",
        "]\n",
        "\n",
        "# Figure showing distributions\n",
        "fig, ax = plt.subplots(figsize=(3, 6.5), nrows=4, ncols=1, sharex=False, sharey=False, dpi=200, layout='constrained')\n",
        "\n",
        "for i in range(0, len(var)):\n",
        "    kde = data[var[i]]\n",
        "    plot = kde.plot.kde(ax=ax[i], style=['k'], linewidth=0.5)\n",
        "    x = plot.get_children()[0]._x\n",
        "    y = plot.get_children()[0]._y\n",
        "    \n",
        "    ax[i].fill_between(x,y,color='gainsboro', alpha=0.7)\n",
        "\n",
        "    # plot cluster points\n",
        "    x_clust = X_test[var[i]][X_test['Cluster_H_7'] == clust]\n",
        "    ax2 = ax[i].twinx()\n",
        "    var_dist = x_clust.plot.kde(ax=ax2, style=['k'], linewidth=0.5)\n",
        "    x_var = var_dist.get_children()[0]._x\n",
        "    y_var = var_dist.get_children()[0]._y\n",
        "    ax2.fill_between(x_var,y_var,color=c, alpha=0.7, label='_nolegend_')\n",
        "\n",
        "    ax2.set_ylabel('')\n",
        "    ax2.set_yticklabels('')\n",
        "    ax2.set_yticks([])\n",
        "    ax2.set_ylim(0)\n",
        "\n",
        "    # Annotation for gray distribution\n",
        "    ax[i].annotate('min={}\\nmax={}'.format(min(kde), max(kde)), dist_coords[i], xycoords='axes fraction', fontsize=8, color='gray')\n",
        "    # Annotation for color distribution\n",
        "    ax[i].annotate('min={}\\nmax={}'.format(min(x_clust), max(x_clust)), anno_coords[i], xycoords='axes fraction', fontsize=8, color=c)\n",
        "\n",
        "for i in range(0, len(var)):\n",
        "    ax[i].set_ylabel('')\n",
        "    ax[i].set_yticklabels('')\n",
        "    ax[i].set_yticks([])\n",
        "    ax[i].set_ylim(0)\n",
        "    ax[i].set_xlim(0)\n",
        "    ax[i].set_xlabel(feat[i], fontsize=8)\n",
        "\n",
        "fig.supylabel('Density', fontsize=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Additional Resources\n",
        "- **Resources**:\n",
        "  - [SHAP GitHub repo](https://github.com/slundberg/shap)\n",
        "  - [Official SHAP documentation and example notebooks](https://shap.readthedocs.io/en/latest/)\n",
        "  - [Original SHAP paper (Lundberg & Lee, 2017)](https://arxiv.org/pdf/1705.07874)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c595441d",
      "metadata": {},
      "source": [
        "### Thank you for attending the SHAP Values Workshop. If you have any questions or comments, don't hesitate to reach out.\n",
        "- Harrison Myers (ghmyers@uvm.edu)\n",
        "- Ryan van der Heijden (ryan.van-der-heijden@uvm.edu)\n",
        "- Ali Dadkhah (ali.dadkhah@uvm.edu)\n",
        "- Shaurya Swami (shaurya.swami@uvm.edu)\n",
        "- Kristen Underwood (kristen.underwood@uvm.edu)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "SHAP",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
